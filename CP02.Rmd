---
title: "CP02"
author: "Luis Vaciero"
date: "9/11/2020"
output: html_document
---
##Cargamos nuestras librerias
```
library(tidyr)
library (boot)
library(rsample)  # data splitting 
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
```
##Limpiamos nuestros datos

```
mData=read.csv("nba.csv")

attach(mData)
#quitamos los % de los titulos que generan problemas
nba <- rename_with(mData, ~ tolower(gsub("%", "", .x, fixed = TRUE)))
#tenemos que limpiar los datos de NaN y duplicados

summarise_all(mData, funs(sum(is.na(.))))
#tenemos NaN en algunas columnas, por lo que hemos de borrarlas
mData <- na.omit(mData)
#tambien tenemos que limpiar los duplicados
nba <- mData[!duplicated(mData$Player), ]
mData
#eliminamos las variables categoricas
borrar <- c("NBA_Country","Tm", "Player")
mData1 <- mData[ , !(names(mData) %in% borrar)]

```


#separamos la muestra en 70% para hacer un sample de training y otro de testing
```
set.seed(123)
data_split <- initial_split(mData, prob = 0.70, strata = Salary) 
data_train <- training(data_split)
data_test  <-  testing(data_split)
```
#Realizamos una regresion elastica con glmnet() 
```{r}
ames_train_x <- model.matrix(Salary ~. -NBA_Country-Tm-Player, data_train)[, -1]
ames_train_y <- log(data_train$Salary)

ames_test_x <- model.matrix(Salary ~. -NBA_Country-Tm-Player, data_test)[, -1]
ames_test_y <- log(data_test$Salary)

#dimension de nuestra matriz
dim(ames_train_x)
```

```{r}
ames_lasso <- glmnet(x = ames_train_x, y = ames_train_y, alpha = 1)
elastic1 <- glmnet(x = ames_train_x, y = ames_train_y, alpha = 0.75) 
elastic2 <- glmnet(x = ames_train_x, y = ames_train_y, alpha = 0.25) 
ames_ridge <- glmnet(x = ames_train_x, y = ames_train_y, alpha = 0.0)

par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1) 
plot(ames_lasso, xvar = "lambda", main = " Alpha = 1\n\n\n")
plot(elastic1, xvar = "lambda", main = "Alpha = 0.75\n\n\n")
plot(elastic2, xvar = "lambda", main = "Alpha = 0.25\n\n\n")
plot(ames_ridge, xvar = "lambda", main = "Alpha = 0\n\n\n")

```

```{r}
fold_id <- sample(1:10, size = length(ames_train_y), replace=TRUE)

# busca las alphas
tuning_grid <- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA
)
tuning_grid

for(i in seq_along(tuning_grid$alpha)) {
  
  # fit CV model for each alpha value
  fit <- cv.glmnet(ames_train_x, ames_train_y, alpha = tuning_grid$alpha[i], foldid = fold_id)
  
  # extract MSE and lambda values
  tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] <- fit$lambda.min
  tuning_grid$lambda_1se[i] <- fit$lambda.1se
}
tuning_grid
  #el lambda mínimo se alcanza cuando alpha = 1, por lo que es un caso de lasso 

#regresion lasso
nba_lasso <- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)
par(mfrow = c(1,1))
plot(nba_lasso, xvar = "lambda") #los coeficientes se acercan a 0 cuanto mayor es lambda

#Con el fin de identificar el valor de λ que da lugar al mejor modelo calculamos el v-test-error utilizan por defecto k=10.
```



```{r}


set.seed(123)

cv_lasso   <- cv.glmnet(ames_train_x, ames_train_y, alpha = 1.0)
plot(cv_lasso) 
#valor de lambda con el que se consigue el mínimo test-error
cv_lasso$lambda.min
#lambda óptimo
cv_lasso$lambda.1se

modelo_lasso <- glmnet(x = ames_train_x, y = ames_train_y, alpha = 1, lambda = cv_lasso$lambda.1se)
coef(modelo_lasso)


pred <- predict(modelo_lasso, s = modelo_lasso$lambda.1se, ames_test_x)
mse_lasso <- sqrt(mean((ames_test_y - pred)^2))
#test error a la hora de predecir el salario
mse_lasso 
```

